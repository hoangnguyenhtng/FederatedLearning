# ü™ü H∆Ø·ªöNG D·∫™N CHO WINDOWS + VSCODE

## ‚úÖ C√ÄI ƒê·∫∂T BAN ƒê·∫¶U (Ch·ªâ l√†m 1 l·∫ßn)

### 1. M·ªü VSCode Terminal

Trong VSCode:
- Nh·∫•n `Ctrl + ~` (m·ªü terminal)
- Ho·∫∑c: View ‚Üí Terminal

**Ch·ªçn PowerShell** (recommended):
- Click dropdown b√™n terminal ‚Üí Select PowerShell

### 2. Activate Virtual Environment

```powershell
# Trong VSCode terminal:
.\fed_rec_env\Scripts\Activate.ps1
```

**N·∫øu g·∫∑p l·ªói "cannot be loaded because running scripts is disabled"**:

```powershell
# Ch·∫°y l·ªánh n√†y (1 l·∫ßn duy nh·∫•t):
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

# Sau ƒë√≥ activate l·∫°i:
.\fed_rec_env\Scripts\Activate.ps1
```

### 3. Install Th√™m Packages

```powershell
pip install sentence-transformers pillow requests tqdm
```

---

## üì• DOWNLOAD DATASET (T·ª± ƒë·ªông)

### Option A: D√πng PowerShell Script (Recommended)

```powershell
# 1. Ch·∫°y script download (t·ª± ƒë·ªông)
PowerShell -ExecutionPolicy Bypass -File download_amazon_data.ps1
```

**Script n√†y s·∫Ω**:
- ‚úÖ T·ª± ƒë·ªông download 2 files (~300MB)
- ‚úÖ T·ª± ƒë·ªông extract
- ‚úÖ Ready to use!

**Th·ªùi gian**: 5-10 ph√∫t (t√πy internet)

---

### Option B: Download Th·ªß C√¥ng (Backup)

N·∫øu script kh√¥ng work, download b·∫±ng browser:

**Step 1: Download files**

1. M·ªü browser, download 2 files n√†y:
   - [Reviews](https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/review_categories/All_Beauty.jsonl.gz) (~200MB)
   - [Metadata](https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/meta_categories/meta_All_Beauty.jsonl.gz) (~100MB)

2. Copy v√†o folder: `D:\Federated Learning\data\raw\amazon_2023\`

**Step 2: Extract files**

```powershell
# Trong VSCode terminal:
cd data\raw\amazon_2023

# Extract reviews
Expand-Archive -Path All_Beauty.jsonl.gz -DestinationPath .

# Extract metadata  
Expand-Archive -Path meta_All_Beauty.jsonl.gz -DestinationPath .
```

**Ho·∫∑c**: Click ph·∫£i file .gz ‚Üí Extract here (n·∫øu c√≥ 7-Zip/WinRAR)

---

## ‚öôÔ∏è PROCESS DATA

### Quick Test (10K samples - Recommended First)

```powershell
# Ch·∫°y processing (trong VSCode terminal)
python src\data_generation\process_amazon_data.py
```

**C·∫•u h√¨nh m·∫∑c ƒë·ªãnh**:
- Sample: 10,000 interactions (ƒë·ªÉ test nhanh)
- Clients: 10
- Output: `data\amazon_2023_processed\client_*\data.pkl`

**Th·ªùi gian ∆∞·ªõc t√≠nh**:
- Loading data: 2-3 ph√∫t
- Processing embeddings: 20-30 ph√∫t (text encoding)
- Downloading images: 10-15 ph√∫t (t√πy internet)
- **Total**: ~40-50 ph√∫t

**Progress s·∫Ω hi·ªán trong terminal**:
```
======================================================================
AMAZON REVIEWS 2023 ‚Üí FEDERATED MULTI-MODAL DATASET
======================================================================
Loading text encoder (SentenceTransformer)...
‚úÖ Initialized processors on device: cuda
Loading data\raw\amazon_2023\All_Beauty.jsonl...
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:02<00:00, 4500.00it/s]
‚úÖ Loaded 10000 records
...
```

---

### Full Dataset (701K samples - Sau khi test OK)

**Edit file**: `src\data_generation\process_amazon_data.py`

T√¨m d√≤ng 371, thay ƒë·ªïi:
```python
# FROM:
SAMPLE_SIZE = 10000  # Process 10K interactions first

# TO:
SAMPLE_SIZE = None  # Process all data
```

**Save** (Ctrl+S) v√† ch·∫°y l·∫°i:
```powershell
python src\data_generation\process_amazon_data.py
```

**Th·ªùi gian**: ~8-12 gi·ªù (ch·∫°y overnight)

---

## üîç KI·ªÇM TRA K·∫æT QU·∫¢

### Xem files ƒë√£ t·∫°o

```powershell
# List processed data
Get-ChildItem -Recurse data\amazon_2023_processed

# K·∫øt qu·∫£ mong ƒë·ª£i:
# client_0\data.pkl
# client_1\data.pkl
# ...
# client_9\data.pkl
```

### Verify data quality

```powershell
# T·∫°o file test
python -c "import pandas as pd; import numpy as np; df = pd.read_pickle('data/amazon_2023_processed/client_0/data.pkl'); print('Samples:', len(df)); print('Text emb shape:', np.array(df['text_embedding'].iloc[0]).shape); print('Image emb shape:', np.array(df['image_embedding'].iloc[0]).shape); print('Behavior shape:', np.array(df['behavior_features'].iloc[0]).shape)"
```

**K·∫øt qu·∫£ mong ƒë·ª£i**:
```
Samples: 1234
Text emb shape: (384,)
Image emb shape: (2048,)
Behavior shape: (32,)
```

---

## üöÄ TRAIN MODEL

### Option 1: S·ª≠ d·ª•ng Amazon Data (Recommended)

**ƒê·∫ßu ti√™n**: Update config ƒë·ªÉ use Amazon data

T·∫°o file m·ªõi: `configs\config_amazon.yaml`

```yaml
# Copy t·ª´ config.yaml, ch·ªâ thay ƒë·ªïi:
paths:
  data_dir: "./data/amazon_2023_processed"  # ‚Üê Use Amazon data
  experiments_dir: "experiments"
  
federated:
  num_clients: 10  # Match processed clients
  num_rounds: 50
  fraction_fit: 0.4
  
training:
  batch_size: 32
  learning_rate: 0.001  # Higher LR for real data
```

**Ch·∫°y training**:
```powershell
python src\training\federated_training_pipeline.py --config configs\config_amazon.yaml
```

---

### Option 2: Quick Fix Synthetic Data (Alternative)

N·∫øu ch∆∞a mu·ªën d√πng Amazon, fix synthetic data tr∆∞·ªõc:

<function_calls>
<invoke name="write">
<parameter name="contents"># Quick Fix for Synthetic Data - Remove Random Noise
# This patches client.py to at least create consistent embeddings

import sys
from pathlib import Path

# Add project root
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

print("=" * 70)
print("QUICK FIX: Synthetic Data Embeddings")
print("=" * 70)

print("\n‚ö†Ô∏è  This is a TEMPORARY fix!")
print("For BEST results, use Amazon data (see AMAZON_DATASET_GUIDE.md)")

# Read client.py
client_file = Path("src/federated/client.py")
with open(client_file, 'r', encoding='utf-8') as f:
    content = f.read()

# Check if already patched
if "QUICK_FIX_APPLIED" in content:
    print("\n‚úÖ Already patched!")
    sys.exit(0)

# Find and replace random text embeddings
old_text = "text_emb = torch.randn(batch_size, 384, device=self.device)"
new_text = """# QUICK_FIX_APPLIED: Use item embeddings from keywords
                    if 'text' in batch_data and batch_data['text']:
                        # Use text encoder if available
                        from sentence_transformers import SentenceTransformer
                        if not hasattr(self, '_text_encoder'):
                            print("Loading text encoder (one-time)...")
                            self._text_encoder = SentenceTransformer('all-MiniLM-L6-v2')
                            self._text_encoder.eval()
                        
                        # Encode text
                        texts = [str(t) if t else "product" for t in batch_data['text']]
                        text_emb = self._text_encoder.encode(texts, convert_to_tensor=True).to(self.device)
                    else:
                        # Fallback: deterministic based on item_id (not random!)
                        item_ids = batch_data.get('item_id', torch.arange(batch_size))
                        # Create deterministic embeddings from item_id
                        text_emb = torch.zeros(batch_size, 384, device=self.device)
                        for i in range(batch_size):
                            seed = int(item_ids[i].item()) if torch.is_tensor(item_ids[i]) else int(item_ids[i])
                            torch.manual_seed(seed)
                            text_emb[i] = torch.randn(384, device=self.device) * 0.1"""

# Replace
if old_text in content:
    content = content.replace(old_text, new_text)
    print("\n‚úÖ Patched text embeddings (deterministic)")
else:
    print("\n‚ö†Ô∏è  Text embedding code not found (may be already modified)")

# Save
with open(client_file, 'w', encoding='utf-8') as f:
    f.write(content)

print("\n‚úÖ PATCH APPLIED!")
print("\nNow you can train with synthetic data (still not as good as Amazon)")
print("Expected accuracy: 40-50% (vs 30% before, 70% with Amazon)")

